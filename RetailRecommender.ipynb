{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import shutil\n","drive.mount('/content/drive')\n","shutil.copyfile(\"/content/drive/MyDrive/RetailRecommender/preprocessing.py\", \"preprocessing.py\")\n","shutil.copyfile(\"/content/drive/MyDrive/RetailRecommender/Datasets/online_retail_processed.csv\", \"sample_data/online_retail_processed.csv\")\n"],"metadata":{"id":"R5SSU_ZoMflj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n","!pip install torch-geometric\n","!pip install sentence-transformers"],"metadata":{"id":"IXp2j1X99zFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtUpYjA3kO6Q"},"outputs":[],"source":["# Initialization\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","import torch_geometric.transforms as T\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.data import Data\n","from sentence_transformers import SentenceTransformer\n","\n","from preprocessing import split_temporal\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","source":["The Dataset"],"metadata":{"id":"JwtXqkskCTj6"}},{"cell_type":"code","source":["df = pd.read_csv('sample_data/online_retail_processed.csv')\n","df.head()"],"metadata":{"id":"yCniskFH6Bzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"ZYKK18DICWLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n","sentence_embeddings = sbert_model.encode(df['Description'])"],"metadata":{"id":"xnZraSBSwQgO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df, test_df = split_temporal(df, \"InvoiceDate\")"],"metadata":{"id":"xDTQwTTCIuaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df"],"metadata":{"id":"0yZNPFl-AOCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RetailData(Dataset):\n","  def __init__(self, df):\n","    x = df.iloc[:,:-1]\n","    y = df.iloc[:, -1]\n","    self.x_train=torch.tensor(x.values,dtype=torch.float32)\n","    self.y_train=torch.tensor(y.values,dtype=torch.float32)\n","    \n","  def __len__(self):\n","    return len(self.y_train)\n","\n","  def __getitem__(self,idx):\n","    return self.x_train[idx],self.y_train[idx]"],"metadata":{"id":"asTKehL-Ny8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.shape, train_df.shape, test_df.shape)\n","print(df['CustomerID'].unique().shape, train_df['CustomerID'].unique().shape, test_df['CustomerID'].unique().shape)\n","print(df['CustomerID'].max(), train_df['CustomerID'].max(), test_df['CustomerID'].max())"],"metadata":{"id":"A4-n5rhWSGkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = RetailData(train_df[[\"Quantity\", \"UnitPrice\", \"CustomerID\", \"CountryID\", \"StockCodeID\"]])"],"metadata":{"id":"94mBY86cbb5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_len = len(df['CustomerID'].unique()) + len(df['StockCodeID'].unique())\n","num_users = len(df['CustomerID'].unique())\n","sentence_embeddings = sbert_model.encode(train_df['Description'])\n","\n","x = torch.tensor(range(0, total_len))\n","edge_index = torch.tensor([train_df['CustomerID'], train_df['StockCodeID'] + num_users]).long()\n","edge_attr = torch.tensor([train_df['UnitPrice'] * train_df['Quantity'], sentence_embeddings]).T\n","avg_edge_weight = edge_attr.mean()\n","print(x.shape, edge_index.shape, edge_attr.shape, avg_edge_weight)\n","tr_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"],"metadata":{"id":"I0ko0ZQo8MuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_index.max()"],"metadata":{"id":"TNj7uHBR3h2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence_embeddings = sbert_model.encode(test_df['Description'])\n","\n","te_x = torch.tensor(range(0, total_len))\n","te_edge_index = torch.tensor([(test_df[\"CustomerID\"]).values, (test_df[\"StockCodeID\"] + num_users).values]).long()\n","te_edge_attr = torch.tensor([test_df['UnitPrice'].values * test_df['Quantity'].values, sentence_embeddings]).T\n","te_data = Data(x=te_x, edge_index=te_edge_index, edge_attr=te_edge_attr)"],"metadata":{"id":"XAv-rELa16Cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(edge_index.shape)\n","print(te_edge_index.shape)"],"metadata":{"id":"dCux2h4YteK0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Customer and Item Nodes"],"metadata":{"id":"5ZjqUNTCCN1g"}},{"cell_type":"code","source":["item_ids = torch.tensor(df['StockCodeID'].unique())\n","customer_ids = torch.tensor(df['CustomerID'].unique())\n","print(item_ids.shape, customer_ids.shape)"],"metadata":{"id":"e9g-kaPy7gP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.utils.negative_sampling import negative_sampling\n","import torch_geometric as ptg\n","import torch.nn.functional as F\n","from torch_geometric.utils import degree"],"metadata":{"id":"5QU4ZjXxHFUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GraphEmbedder(nn.Module):\n","  def __init__(self, embed_size, num_layers, num_nodes, dropout_p=0.5):\n","    super().__init__()\n","    self.embed = nn.Embedding(num_nodes, embed_size)\n","    layers = [ptg.nn.GCNConv(embed_size, embed_size) for _ in range(num_layers)]\n","    self.layers = nn.ModuleList(layers)\n","    self.dropout_p = dropout_p\n","  \n","  def forward(self, data):\n","    x,edge_indices = data.x, data.edge_index\n","    x = self.embed(x)\n","    for i, layer in enumerate(self.layers):\n","      dropped = F.dropout(x, p=self.dropout_p, training=self.training)\n","      x = layer(dropped, edge_indices)\n","      if i != len(self.layers) - 1:\n","        x = F.relu(x)\n","    return x"],"metadata":{"id":"I1I1jTroIRRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recall_k(net, data, k):\n","  net.eval()\n","  embeds = net(data)\n","  user_embeds, item_embeds = embeds[:num_users], embeds[num_users:]\n","  dots = F.logsigmod(user_embeds @ item_embeds.T)\n","  top_k = torch.topk(dots, dim=1).indices\n","  neighs = degree(data.edge_index[0])\n","  total = 0\n","  for u in range(num_users):\n","    overlap_amt = len(np.intersect1d(top_k[u].detach().cpu().numpy(),torch.where(data.edge_index[0] == u, data.edge_index[1], -1).detach().cpu().numpy()))\n","    total += overlap_amt/neighs[u]\n","  return total/num_users"],"metadata":{"id":"v6KM3IxZt-oX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(encoder, optim, epochs, tr_data, te_data):\n","    tr_losses = []\n","    te_losses = []\n","    te_recalls = []\n","    best_recall = -np.inf\n","    bad_epochs = 0\n","    for i in range(epochs):\n","      embedded = encoder(tr_data)\n","      user_embeds, item_embeds = embedded[:num_users], embedded[num_users:]\n","      pos = tr_data.edge_index\n","      neg = negative_sampling(tr_data.edge_index)\n","      dots = user_embeds @ item_embeds.T\n","      pos_weights = F.logsigmoid(dots[pos[0], pos[1]-num_users]*tr_data.edge_attr[0])\n","      neg_weights = F.logsigmoid(dots[neg[0], neg[1]-num_users]*avg_edge_weight)\n","\n","      loss = pos_weights.sum() - neg_weights.sum()\n","      loss.backward()\n","      optim.step()\n","      optim.zero_grad()\n","\n","      # epoch testing\n","      embedded = encoder(te_data)\n","      user_embeds, item_embeds = embedded[:num_users], embedded[num_users:]\n","      pos = te_data.edge_index\n","      neg = negative_sampling(te_data.edge_index)\n","      dots = user_embeds @ item_embeds.T\n","      pos_weights = F.logsigmoid(dots[pos[0], pos[1]-num_users]*te_data.edge_attr[0])\n","      neg_weights = F.logsigmoid(dots[neg[0], neg[1]-num_users]*avg_edge_weight)\n","\n","      te_loss = pos_weights.sum() - neg_weights.sum()\n","      \n","      tr_losses.append(loss.item())\n","      te_recalls.append(te_loss.item())\n","      te_recalls.append(recall_k(encoder, te_data, 150).item())\n","      if i % 10 == 9:\n","          print(tr_losses[-1])\n","    fig, (ax1, ax2, ax3) = plt.subplots(2)\n","    fig.set_figheight(15)\n","    fig.set_figwidth(15)\n","    ax1.plot(tr_losses, label=\"Train Loss\", color=\"#cc6462\")\n","    ax1.plot(te_losses, label=\"Valid Loss\", color=\"#9e9e9e\") \n","    ax1.set(xlabel=\"Iterations\")\n","    ax1.set_title(\"Training and Validation Losses\")\n","    ax1.legend()\n","\n","    ax3.plot(te_recalls, color=\"#cc6462\")\n","    ax3.set_title(\"Recall 150\")"],"metadata":{"id":"grlTGzexKSr4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_net = GraphEmbedder(32, 2, total_len)\n","optimizer = torch.optim.Adam(embed_net.parameters())"],"metadata":{"id":"kXrHtyWfsoCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(embed_net, optimizer, 200, tr_data, None)"],"metadata":{"id":"ZxQtbnbctHZB"},"execution_count":null,"outputs":[]}]}